from sparseattn.efficiency.model.modeling_flash_qwen_streaming_decode import PawQwen3ForCausalLM, PawQwen3Config
from sparseattn.efficiency.model.modeling_flash_qwen import PawQwen3ForCausalLM as PawQwen3ForCausalLM_full_decode
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json
import pandas as pd
import gc
import os
from datetime import datetime

def truncate_input_ids(input_ids, max_len):
    """å°†input_idsæˆªæ–­åˆ°æŒ‡å®šé•¿åº¦ï¼Œä¿ç•™å¤´éƒ¨ä¸€åŠå’Œå°¾éƒ¨ä¸€åŠ"""
    seq_len = input_ids.shape[-1]
    
    if seq_len <= max_len:
        return input_ids, seq_len, ""
    
    # ç­–ç•¥ï¼šä¿ç•™å¤´éƒ¨ä¸€åŠé…é¢ï¼Œä¿ç•™å°¾éƒ¨ä¸€åŠé…é¢ï¼Œä¸­é—´åˆ‡æ‰
    half_len = max_len // 2
    
    # 1. å–å‰ half_len
    head_part = input_ids[:, :half_len]
    
    # 2. å–å (max - half) (ä¸ºäº†å¤„ç†å¥‡æ•°é•¿åº¦çš„æƒ…å†µ)
    tail_part = input_ids[:, -(max_len - half_len):]
    
    # 3. æ‹¼æ¥
    truncated_input_ids = torch.cat([head_part, tail_part], dim=1)
    
    note = f"âœ‚ï¸ (Mid-Trunc {seq_len} -> {max_len})"
    return truncated_input_ids, max_len, note

def test_model_decode(model, model_name, tokenizer, input_ids, gen_len=10):
    """æµ‹è¯•æ¨¡å‹decodeé˜¶æ®µçš„æ—¶é—´é—´éš”"""
    print(f"  Testing {model_name}...")
    
    # Prefillé˜¶æ®µ
    device = next(model.parameters()).device
    if input_ids.device != device:
        input_ids = input_ids.to(device)
    
    with torch.inference_mode():
        outputs = model(input_ids, use_cache=True)
    
    past_key_values = outputs.past_key_values
    next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(1)
    
    # Decodeé˜¶æ®µ - æ”¶é›†æ—¶é—´æ•°æ®
    decode_times = []
    
    with torch.inference_mode():
        for i in range(gen_len):
            outputs = model(next_token, past_key_values=past_key_values, use_cache=True)
            past_key_values = outputs.past_key_values
            next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(1)
            
            # ä¿å­˜æ—¶é—´é—´éš”æ•°æ®
            if hasattr(outputs, 'time_intervals'):
                decode_times.append(outputs.time_intervals.copy())
            else:
                decode_times.append({})
    
    return decode_times

def analyze_decode_times(decode_times_list, model_type, gen_len=10):
    """åˆ†ædecodeé˜¶æ®µçš„æ—¶é—´æ•°æ®ï¼Œè¿”å›æ¯ä¸ªtokençš„å¹³å‡æ—¶é—´(ms)"""
    if model_type == 'streaming':
        # å¯¹äºstreamingæ¨¡å‹ï¼Œè®¡ç®—æ€»æ—¶é—´ç„¶åå¹³å‡åˆ°æ¯ä¸ªtoken
        total_times = {
            'sparse_prep_time': 0,
            'sparse_attn_time': 0,
            'full_prep_time': 0,
            'full_attn_time': 0
        }
        
        if not decode_times_list:
            return total_times
            
        for times in decode_times_list:
            for key in total_times.keys():
                # ç´¯åŠ æ‰€æœ‰decodeæ­¥éª¤çš„æ—¶é—´ï¼ˆç§’ï¼‰
                total_times[key] += times.get(key, 0)
        
        # è®¡ç®—æ¯ä¸ªtokençš„å¹³å‡æ—¶é—´ï¼Œå¹¶è½¬æ¢ä¸ºæ¯«ç§’
        avg_times = {}
        for key in total_times.keys():
            # æ€»æ—¶é—´é™¤ä»¥tokenæ•°ï¼Œç„¶åè½¬æ¢ä¸ºæ¯«ç§’ (1s = 1000ms)
            avg_times[key] = (total_times[key] / gen_len) * 1000 if gen_len > 0 else 0
        
        return avg_times
    
    elif model_type == 'full_decode':
        # å¯¹äºfull_decodeæ¨¡å‹
        total_time = 0
        
        if not decode_times_list:
            return {'flash_attn_decode_time': 0}
            
        for times in decode_times_list:
            total_time += times.get('flash_attn_decode_time', 0)
        
        # è®¡ç®—æ¯ä¸ªtokençš„å¹³å‡æ—¶é—´ï¼Œå¹¶è½¬æ¢ä¸ºæ¯«ç§’
        avg_time = (total_time / gen_len) * 1000 if gen_len > 0 else 0
        
        return {'flash_attn_decode_time': avg_time}

def cleanup_memory():
    """æ¸…ç†GPUå’Œå†…å­˜"""
    torch.cuda.empty_cache()
    gc.collect()

if __name__ == "__main__":
    model_path = "/data1/lcm_lab/qqt/SparseAttn/sparseattn/checkpoints/1.1router4steps266_full_streaming_64k_qwen3-4b_wfrozen/checkpoint-230"
    data_path = "/data1/lcm_lab/sora/loomeval/benchmarks/General/RULER/data/niah_single_3_262144.jsonl"
    
    num_samples = 5       # æ¯ä¸ªé•¿åº¦æµ‹è¯•çš„æ ·æœ¬æ•°
    gen_len = 10          # decodeé˜¶æ®µçš„ç”Ÿæˆé•¿åº¦
    
    target_lengths_k = [32]  # å¯ä»¥æ‰©å±•æ›´å¤šé•¿åº¦
    target_lengths = [k * 1024 for k in target_lengths_k] 
    
    # è¯»å–æ•°æ®
    print(f"ğŸ“‚ [Init] Reading data from {data_path}")
    samples = []
    with open(data_path, 'r') as f:
        for i, line in enumerate(f):
            if len(samples) >= num_samples: 
                break
            try:
                item = json.loads(line)
                samples.append(item)
            except:
                pass
    
    print(f"ğŸ“Š Loaded {len(samples)} samples for testing")
    print(f"ğŸ”§ Config: {gen_len} decode tokens per sample")
    
    # å‡†å¤‡ç»“æœå­˜å‚¨
    results = []
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # ==================== ç¬¬ä¸€é˜¶æ®µï¼šæµ‹è¯•Full Decodeæ¨¡å‹ ====================
    print("\n" + "="*60)
    print("ğŸ” [Phase 1] Testing Full Decode Model")
    print("="*60)
    
    print("ğŸ”„ [Loading] Loading full decode model...")
    model_full = PawQwen3ForCausalLM_full_decode.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda:0",
        trust_remote_code=True,
    )
    model_full.eval()
    
    # å­˜å‚¨full decodeæ¨¡å‹çš„ç»“æœ
    full_decode_results = {}
    
    # å¯¹æ¯ä¸ªç›®æ ‡é•¿åº¦è¿›è¡Œæµ‹è¯•
    for max_len in target_lengths:
        print(f"\nğŸ“ [Testing] Testing full decode with max length: {max_len} tokens")
        
        for i, item in enumerate(samples):
            # è·å–è¾“å…¥æ–‡æœ¬
            input_text = item.get("input", item.get("text", item.get("content", "")))
            if not input_text:
                # å°è¯•ç¬¬ä¸€ä¸ªé”®å€¼å¯¹
                keys = list(item.keys())
                if keys:
                    input_text = str(item[keys[0]])
                else:
                    continue
            
            # ç¼–ç å¹¶æˆªæ–­
            input_ids = tokenizer.encode(input_text, return_tensors="pt")
            input_ids, seq_len, note = truncate_input_ids(input_ids, max_len)
            
            print(f"  Sample {i+1}: {seq_len} tokens {note}")
            
            # æµ‹è¯•full decodeæ¨¡å‹
            full_times = test_model_decode(model_full, "Full Decode", tokenizer, input_ids, gen_len)
            full_avg = analyze_decode_times(full_times, 'full_decode', gen_len)
            
            # å­˜å‚¨ç»“æœ
            key = (seq_len, i)
            full_decode_results[key] = {
                'flash_attn_decode_time': full_avg.get('flash_attn_decode_time', 0)
            }
    
    # æ¸…ç†full decodeæ¨¡å‹
    print("\nğŸ§¹ [Cleaning] Cleaning up full decode model...")
    del model_full
    cleanup_memory()
    
    # ==================== ç¬¬äºŒé˜¶æ®µï¼šæµ‹è¯•Streaming Decodeæ¨¡å‹ ====================
    print("\n" + "="*60)
    print("ğŸ” [Phase 2] Testing Streaming Decode Model")
    print("="*60)
    
    print("ğŸ”„ [Loading] Loading streaming decode model...")
    model_streaming = PawQwen3ForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda:0",
        trust_remote_code=True,
    )
    model_streaming.eval()
    
    # å¯¹æ¯ä¸ªç›®æ ‡é•¿åº¦è¿›è¡Œæµ‹è¯•
    for max_len in target_lengths:
        print(f"\nğŸ“ [Testing] Testing streaming decode with max length: {max_len} tokens")
        
        for i, item in enumerate(samples):
            # è·å–è¾“å…¥æ–‡æœ¬
            input_text = item.get("input", item.get("text", item.get("content", "")))
            if not input_text:
                # å°è¯•ç¬¬ä¸€ä¸ªé”®å€¼å¯¹
                keys = list(item.keys())
                if keys:
                    input_text = str(item[keys[0]])
                else:
                    continue
            
            # ç¼–ç å¹¶æˆªæ–­ï¼ˆä¸ç¬¬ä¸€é˜¶æ®µä¿æŒç›¸åŒçš„æˆªæ–­æ–¹å¼ï¼‰
            input_ids = tokenizer.encode(input_text, return_tensors="pt")
            input_ids, seq_len, note = truncate_input_ids(input_ids, max_len)
            
            print(f"  Sample {i+1}: {seq_len} tokens {note}")
            
            # æµ‹è¯•streamingæ¨¡å‹
            streaming_times = test_model_decode(model_streaming, "Streaming Decode", tokenizer, input_ids, gen_len)
            streaming_avg = analyze_decode_times(streaming_times, 'streaming', gen_len)
            
            # åˆå¹¶ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ
            key = (seq_len, i)
            full_result = full_decode_results.get(key, {'flash_attn_decode_time': 0})
            
            result = {
                'prefill_length': seq_len,
                'sample_idx': i,
                'flash_attn_decode_time': full_result['flash_attn_decode_time'],
                'sparse_prep_time': streaming_avg.get('sparse_prep_time', 0),
                'sparse_attn_time': streaming_avg.get('sparse_attn_time', 0),
                'full_prep_time': streaming_avg.get('full_prep_time', 0),
                'full_attn_time': streaming_avg.get('full_attn_time', 0)
            }
            results.append(result)
    
    # æ¸…ç†streamingæ¨¡å‹
    print("\nğŸ§¹ [Cleaning] Cleaning up streaming decode model...")
    del model_streaming
    cleanup_memory()
    
    # ==================== ç»“æœå¤„ç† ====================
    print("\n" + "="*60)
    print("ğŸ“Š [Processing] Processing results")
    print("="*60)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(results)
    
    # æŒ‰prefill_lengthåˆ†ç»„è®¡ç®—å¹³å‡å€¼
    if not df.empty:
        df_avg = df.groupby('prefill_length').mean().reset_index()
        df_avg = df_avg.round(3)  # ä¿ç•™3ä½å°æ•°ï¼Œæ¯«ç§’çº§åˆ«ç²¾åº¦è¶³å¤Ÿ
        
        # è®¡ç®—speedupåˆ—ï¼šflash_attn_decode_time / (sparse_attn_time + full_attn_time)
        # æ³¨æ„ï¼šè¿™é‡Œåªæ¯”è¾ƒæ³¨æ„åŠ›è®¡ç®—æ—¶é—´ï¼Œä¸åŒ…æ‹¬prepæ—¶é—´
        df_avg['speedup'] = df_avg.apply(
            lambda row: row['flash_attn_decode_time'] / (row['sparse_attn_time'] + row['full_attn_time']) 
            if (row['sparse_attn_time'] + row['full_attn_time']) > 0 else 0,
            axis=1
        )
        df_avg['speedup'] = df_avg['speedup'].round(2)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†speedupæ”¾åœ¨æœ€å³è¾¹
        column_order = ['prefill_length', 'flash_attn_decode_time', 'sparse_prep_time', 
                        'sparse_attn_time', 'full_prep_time', 'full_attn_time', 'speedup']
        df_avg = df_avg[column_order]
        
        # æŒ‰prefill_lengthæ’åº
        df_avg = df_avg.sort_values('prefill_length')
        
        # ==================== ä¿å­˜CSVæ–‡ä»¶ ====================
        # ç”Ÿæˆæ—¶é—´æˆ³ï¼Œé¿å…æ–‡ä»¶åå†²çª
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ä¸»è¦ç»“æœCSVæ–‡ä»¶ - æ¯ä¸ªprefillé•¿åº¦çš„å¹³å‡å€¼ï¼ˆåŒ…å«speedupåˆ—ï¼‰
        main_csv = f"decode_time_comparison_ms_{timestamp}.csv"
        df_avg.to_csv(main_csv, index=False)
        
        print(f"\nâœ… [CSV Saved] Main results saved to: {main_csv}")
        print(f"   - æ–‡ä»¶ä½ç½®: {os.path.abspath(main_csv)}")
        print(f"   - æ•°æ®è¡Œæ•°: {len(df_avg)} (æ¯ä¸ªprefillé•¿åº¦ä¸€è¡Œ)")
        print(f"   - æ•°æ®åˆ—æ•°: {len(df_avg.columns)}")
        print(f"   - åŒ…å«åˆ—: {', '.join(df_avg.columns.tolist())}")
        print(f"   - Speedupå®šä¹‰: flash_attn_decode_time / (sparse_attn_time + full_attn_time)")
        print(f"     * speedup > 1: Full decodeæ›´æ…¢")
        print(f"     * speedup < 1: Streaming decodeæ›´æ…¢")
        
        # 2. è¯¦ç»†ç»“æœCSVæ–‡ä»¶ - æ¯ä¸ªæ ·æœ¬çš„åŸå§‹æ•°æ®
        detailed_csv = f"detailed_decode_results_ms_{timestamp}.csv"
        df.to_csv(detailed_csv, index=False)
        print(f"\nğŸ“‹ [CSV Saved] Detailed results saved to: {detailed_csv}")
        print(f"   - æ–‡ä»¶ä½ç½®: {os.path.abspath(detailed_csv)}")
        print(f"   - æ•°æ®è¡Œæ•°: {len(df)} (æ¯ä¸ªæ ·æœ¬ä¸€è¡Œ)")
        print(f"   - åŒ…å«sample_idxåˆ—ç”¨äºè¿½è¸ªåŸå§‹æ ·æœ¬")
        
        # 3. ç»Ÿè®¡æ‘˜è¦CSVæ–‡ä»¶ - æ±‡æ€»ç»Ÿè®¡
        summary_stats = {
            'total_samples': len(df),
            'unique_prefill_lengths': len(df_avg),
            'gen_len_per_sample': gen_len,
            'test_timestamp': timestamp,
            'data_source': os.path.basename(data_path),
            'model_path': os.path.basename(model_path)
        }
        
        # ä¸ºæ¯ä¸ªæ—¶é—´æŒ‡æ ‡æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        time_columns = ['flash_attn_decode_time', 'sparse_prep_time', 'sparse_attn_time', 
                       'full_prep_time', 'full_attn_time', 'speedup']
        
        stats_df = pd.DataFrame([summary_stats])
        for col in time_columns:
            if col in df_avg.columns:
                if col == 'speedup':
                    # å¯¹äºspeedupï¼Œè®¡ç®—å‡ ä½•å¹³å‡å€¼å¯èƒ½æ›´æœ‰æ„ä¹‰
                    stats_df[f'{col}_mean'] = df_avg[col].mean()
                    stats_df[f'{col}_min'] = df_avg[col].min()
                    stats_df[f'{col}_max'] = df_avg[col].max()
                else:
                    stats_df[f'{col}_mean'] = df_avg[col].mean()
                    stats_df[f'{col}_min'] = df_avg[col].min()
                    stats_df[f'{col}_max'] = df_avg[col].max()
                    stats_df[f'{col}_std'] = df_avg[col].std()
        
        stats_csv = f"summary_stats_ms_{timestamp}.csv"
        stats_df.to_csv(stats_csv, index=False)
        print(f"\nğŸ“ˆ [CSV Saved] Summary statistics saved to: {stats_csv}")
        
        print("\nğŸ“Š Summary Table (å•ä½: ms/token):")
        print(df_avg.to_string(index=False))
        
        print(f"\nğŸ“ Note: All times are averaged over {gen_len} decode tokens (ms per token)")
        print(f"\nğŸ’¾ [File Summary] Saved CSV files:")
        print(f"   1. {main_csv} - ä¸»è¦ç»“æœ (æŒ‰prefillé•¿åº¦åˆ†ç»„å¹³å‡å€¼ï¼ŒåŒ…å«speedupåˆ—)")
        print(f"   2. {detailed_csv} - è¯¦ç»†ç»“æœ (æ¯ä¸ªæ ·æœ¬çš„åŸå§‹æ•°æ®)")
        print(f"   3. {stats_csv} - ç»Ÿè®¡æ‘˜è¦ (æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯)")
        
        # ç®€å•æ€»ç»“speedupç»“æœ
        print(f"\nğŸš€ [Speedup Summary]:")
        avg_speedup = df_avg['speedup'].mean()
        min_speedup = df_avg['speedup'].min()
        max_speedup = df_avg['speedup'].max()
        
        print(f"   - Average speedup: {avg_speedup:.2f}x")
        print(f"   - Min speedup: {min_speedup:.2f}x")
        print(f"   - Max speedup: {max_speedup:.2f}x")
        
        if avg_speedup > 1:
            print(f"   - Overall: Full decode is {avg_speedup:.1f}x slower than streaming decode")
        else:
            print(f"   - Overall: Streaming decode is {1/avg_speedup:.1f}x slower than full decode" if avg_speedup > 0 else "   - Cannot compare")
        
    else:
        print("âŒ No valid results collected!")