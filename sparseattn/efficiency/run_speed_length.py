import json
import torch
import time
import os
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd
import datetime
# -----------------------------------------------------------------------------
# 1. ç»Ÿä¸€æ¨¡å‹åŠ è½½å™¨
# -----------------------------------------------------------------------------
def load_model(model_path, is_sparse):
    print(f"\nğŸ“¥ [System] Loading model from: {model_path} ...")
    config_path = f"{model_path}/config.json"
    if not os.path.exists(config_path):
        raise ValueError(f"âŒ Config not found at {config_path}")
        
    with open(config_path, "r") as f:
        config_data = json.load(f)

    archs = config_data.get("architectures", [])
    arch_name = archs[0] if archs else "Unknown"
    print(f"ğŸ—ï¸  [System] Detected architecture: {arch_name}")

    if is_sparse:
        # --- è‡ªå®šä¹‰ Sparse æ¨¡å‹æ³¨å†Œé€»è¾‘ ---
        if "PawLlama" in arch_name:
            from sparseattn.training.eval.modeling_flash_llama import (
                PawLlamaForCausalLM, PawLlamaConfig
            )
            AutoModelForCausalLM.register(PawLlamaConfig, PawLlamaForCausalLM)
            model_cls = PawLlamaForCausalLM
        elif "PawQwen" in arch_name:
            from sparseattn.efficiency.model.modeling_flash_qwen import (
                PawQwen3ForCausalLM, PawQwen3Config
            )
            # from sparseattn.efficiency.model.modeling_flash_qwen_prulong import (
            #     PawQwen3ForCausalLM, PawQwen3Config
            # )
            AutoModelForCausalLM.register(PawQwen3Config, PawQwen3ForCausalLM)
            model_cls = PawQwen3ForCausalLM
        # else:
        #     # --- æ ‡å‡† Full Attention æ¨¡å‹ (å¦‚ Qwen2/3) ---
        #     print("â„¹ï¸  [System] Loading as Standard (Full Attention) Model.")
        #     model_cls = AutoModelForCausalLM
        #     is_sparse = False
    else:
        if "PawLlama" in arch_name:
            from sparseattn.training.eval.modeling_flash_llama import (
                PawLlamaForCausalLM, PawLlamaConfig
            )
            AutoModelForCausalLM.register(PawLlamaConfig, PawLlamaForCausalLM)
            model_cls = PawLlamaForCausalLM
        elif "PawQwen" in arch_name:
            from sparseattn.efficiency.model.modeling_flash_qwen_full import (
                PawQwen3ForCausalLM, PawQwen3Config
            )
            AutoModelForCausalLM.register(PawQwen3Config, PawQwen3ForCausalLM)
            model_cls = PawQwen3ForCausalLM

    model = model_cls.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda:0",
        trust_remote_code=True,
    )
    model.eval()
    return model, is_sparse

# -----------------------------------------------------------------------------
# 2. æ ¸å¿ƒè¯„æµ‹å‡½æ•°
# -----------------------------------------------------------------------------
def evaluate_efficiency(model, input_ids, gen_len=10, is_sparse=False):
    # è®¡æ—¶å™¨åˆå§‹åŒ–
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    
    # --- A. Prefill é˜¶æ®µ ---
    torch.cuda.synchronize()
    start_event.record()
    
    with torch.inference_mode():
        outputs = model(input_ids, use_cache=True)
        
    end_event.record()
    torch.cuda.synchronize()
    prefill_time_ms = start_event.elapsed_time(end_event)
    
    past_key_values = outputs.past_key_values
    
    # è·å– Sparsity (ä»… Sparse æ¨¡å‹æœ‰)
    current_sparsity = 0.0
    if is_sparse:
        try:
            sp = getattr(model, "prefill_sparsity", None)
            if isinstance(sp, torch.Tensor):
                current_sparsity = sp.item()
        except:
            pass

    # å‡†å¤‡ Decode
    next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(1)
    
    # --- B. Decode é˜¶æ®µ ---
    torch.cuda.synchronize()
    start_event.record()
    
    with torch.inference_mode():
        for _ in range(gen_len):
            outputs = model(next_token, past_key_values=past_key_values, use_cache=True)
            past_key_values = outputs.past_key_values
            next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(1).to(model.device).contiguous()
            
    end_event.record()
    torch.cuda.synchronize()
    decode_time_ms = start_event.elapsed_time(end_event)
    
    return {
        "prefill_ms": prefill_time_ms,
        "decode_ms_total": decode_time_ms,
        "decode_ms_per_token": decode_time_ms / gen_len,
        "sparsity": current_sparsity
    }

# -----------------------------------------------------------------------------
# 3. æ‰¹é‡æµ‹è¯•æ‰§è¡Œå™¨ (ä¿®æ”¹ï¼šæˆªæ–­é€»è¾‘)
# -----------------------------------------------------------------------------
def run_benchmark_suite(model_path, samples, tokenizer, gen_len=10, max_len=4096,
                        is_sparse=False):
    model, is_sparse = load_model(model_path, is_sparse)
    results = []
    
    # Warmup
    # print("ğŸ”¥ [System] Warming up GPU...")
    # dummy = tokenizer.encode("Warmup " * 10, return_tensors="pt").to(model.device)
    # evaluate_efficiency(model, dummy, gen_len=2, is_sparse=is_sparse)
    # breakpoint()
    
    print(f"ğŸƒ [System] Running benchmark on {len(samples)} samples (Max Len: {max_len})...")
    
    for i, item in enumerate(samples):
        input_text = item["input"]
        input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)
        seq_len = input_ids.shape[-1]
        
        note = ""
        # === æˆªæ–­é€»è¾‘ ===
        if seq_len > max_len:
            # ç­–ç•¥ï¼šä¿ç•™å¤´éƒ¨ä¸€åŠé…é¢ï¼Œä¿ç•™å°¾éƒ¨ä¸€åŠé…é¢ï¼Œä¸­é—´åˆ‡æ‰
            # è¿™æ ·åªè¦ max_input_len >= 200ï¼Œå°±ç»å¯¹èƒ½ä¿ç•™å‰100å’Œå100
            half_len = max_len // 2
            
            # 1. å–å‰ half_len (åŒ…å«å‰100)
            head_part = input_ids[:, :half_len]
            
            # 2. å–å (max - half) (åŒ…å«å100)
            # æ³¨æ„ï¼šç”¨ (max - half) è€Œä¸æ˜¯ half æ˜¯ä¸ºäº†å¤„ç†å¥‡æ•°é•¿åº¦çš„æƒ…å†µ
            tail_part = input_ids[:, -(max_len - half_len):]
            
            # 3. æ‹¼æ¥
            input_ids = torch.cat([head_part, tail_part], dim=1)
            
            note = f"âœ‚ï¸ (Mid-Trunc {seq_len} -> {max_len})"
            seq_len = max_len
        # ===============
            
        res = evaluate_efficiency(model, input_ids, gen_len=gen_len, is_sparse=is_sparse)
        # è·³è¿‡ç¬¬ä¸€ä¸ªé¢„çƒ­é˜¶æ®µ
        if i == 0:
            print("ğŸ”¥(Skipping warmup sample)")
            continue
        
        res["seq_len"] = seq_len
        results.append(res)
        
        # å®æ—¶æ‰“å°è¿›åº¦
        print(f"  Sample {i} {note}: ğŸ“ Len {seq_len} | âš¡ Prefill {res['prefill_ms']:.1f}ms | â© Decode {res['decode_ms_per_token']:.2f}ms/tok")
        
        del input_ids
    
    # æ¸…ç†æ¨¡å‹é‡Šæ”¾æ˜¾å­˜
    del model
    torch.cuda.empty_cache()
    gc.collect()
    print("ğŸ§¹ [System] Model unloaded & Memory cleared.\n")
    
    return results

# -----------------------------------------------------------------------------
# 4. ä¸»ç¨‹åº
# -----------------------------------------------------------------------------
def main():
    # ================= é…ç½®åŒºåŸŸ =================
    # sparse_model_path = "/data1/lcm_lab/qqt/SparseAttn/sparseattn/checkpoints/1.1router4steps266_full_streaming_64k_qwen3-4b_wfrozen/checkpoint-230"
    sparse_model_path = "/data2/hf_models/prulong_qwen_3_4b/"
    # sparse_model_path = ""
    full_model_path   = "/data1/lcm_lab/qqt/SparseAttn/sparseattn/checkpoints/1.1router4steps266_full_streaming_64k_qwen3-4b_wfrozen/checkpoint-200"
    
    data_path = "/data1/lcm_lab/sora/loomeval/benchmarks/General/RULER/data/niah_single_3_262144.jsonl"
    
    num_samples = 5       # æ¯ä¸ªé•¿åº¦æµ‹è¯•çš„æ ·æœ¬æ•°
    gen_len = 1           # ç”Ÿæˆé•¿åº¦
    
    target_lengths_k = [8, 16, 32, 64, 128]
    target_lengths = [k * 1024 for k in target_lengths_k] 

    # 1. å‡†å¤‡æ•°æ®
    print(f"ğŸ“‚ [Init] Reading data from {data_path}")
    tokenizer = AutoTokenizer.from_pretrained(sparse_model_path, trust_remote_code=True)
    
    raw_samples = []
    with open(data_path, 'r') as f:
        for i, line in enumerate(f):
            if len(raw_samples) >= num_samples + 1: break
            try:
                raw_samples.append(json.loads(line))
            except: pass
            
    if not raw_samples:
        print("âŒ Error: No data loaded.")
        return

    all_excel_records = []

    # === å¤–å±‚å¾ªç¯ï¼šéå†ä¸åŒé•¿åº¦ ===
    for target_len in target_lengths:
        print("\n" + "â–ˆ" * 80)
        print(f"ğŸ¯ TARGET LENGTH: {target_len} ({target_len/1024:.0f}K)")
        print("â–ˆ" * 80)

        print(f"ğŸ”¸ Full Model @ {target_len}")
        full_results = run_benchmark_suite(full_model_path, raw_samples, tokenizer, gen_len, target_len, False)

        print(f"ğŸ”¹ Sparse Model @ {target_len}")
        sparse_results = run_benchmark_suite(sparse_model_path, raw_samples, tokenizer, gen_len, target_len, True)

        print("\n" + "ğŸ“Š" * 15 + f" COMPARISON REPORT ({target_len/1024:.0f}K) " + "ğŸ“Š" * 15)
        print(f"{'ID':<4} | {'Len':<6} | {'Sparse (ms)':<18} | {'Full (ms)':<18} | {'ğŸš€ Speedup (Full/Sparse)':<22}")
        print(f"{'':<4} | {'':<6} | {'âš¡Prefill':<9} {'â©Decode':<8} | {'âš¡Prefill':<9} {'â©Decode':<8} | {'âš¡Prefill':<9} {'â©Decode':<8}")
        print("-" * 100)

        avg_speedup_prefill = []
        avg_speedup_decode = []
        
        min_len = min(len(full_results), len(sparse_results))

        for i in range(min_len):
            res_s = sparse_results[i]
            res_f = full_results[i]
            
            len_tok = res_s['seq_len']
            
            # æå–æŒ‡æ ‡
            p_s, d_s = res_s['prefill_ms'], res_s['decode_ms_per_token']
            p_f, d_f = res_f['prefill_ms'], res_f['decode_ms_per_token']
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup_p = p_f / p_s if p_s > 0 else 0
            speedup_d = d_f / d_s if d_s > 0 else 0
            
            avg_speedup_prefill.append(speedup_p)
            avg_speedup_decode.append(speedup_d)
            
            # === è¿™é‡Œä¿ç•™ä½ çš„é«˜äº®ä¸ Emoji é€»è¾‘ ===
            sp_p_str = f"\033[92m{speedup_p:<8.2f}x\033[0m" if speedup_p > 1.0 else f"{speedup_p:<8.2f}x"
            sp_d_str = f"\033[92m{speedup_d:<8.2f}x\033[0m" if speedup_d > 1.0 else f"{speedup_d:<8.2f}x"
            
            if speedup_p > 1.0: sp_p_str += "ğŸ”¥"
            if speedup_d > 1.0: sp_d_str += "ğŸ”¥"

            print(f"{i+1:<4} | {len_tok:<6} | {p_s:<9.1f} {d_s:<8.2f} | {p_f:<9.1f} {d_f:<8.2f} | {sp_p_str} {sp_d_str}")

            # === åŒæ—¶æ”¶é›† Excel æ•°æ® ===
            all_excel_records.append({
                "Target Length (K)": f"{target_len/1024:.0f}K",
                "Actual Seq Len": len_tok,
                "Sample ID": i + 1,
                "Sparse Prefill (ms)": p_s,
                "Sparse Decode (ms)": d_s,
                "Full Prefill (ms)": p_f,
                "Full Decode (ms)": d_f,
                "Speedup Prefill": speedup_p,
                "Speedup Decode": speedup_d,
                "Sparsity": res_s.get('sparsity', 0)
            })

        print("-" * 100)
        
        if avg_speedup_prefill:
            print(f"âœ¨ Average Speedup -> Prefill: \033[1m{sum(avg_speedup_prefill)/len(avg_speedup_prefill):.2f}x\033[0m")
            print(f"âœ¨ Average Speedup -> Decode : \033[1m{sum(avg_speedup_decode)/len(avg_speedup_decode):.2f}x\033[0m")
            
            valid_sp = [r['sparsity'] for r in sparse_results if r]
            if valid_sp:
                avg_spa = sum(valid_sp)/len(valid_sp)
                print(f"ğŸ“‰ Average Sparse Rate: {avg_spa:.4f}")
        else:
            print("âš ï¸ No valid samples comparing.")

    if all_excel_records:
        ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        file_name = f"/data1/lcm_lab/qqt/SparseAttn/sparseattn/efficiency/results/benchmark_length_{ts}.xlsx"
        print(f"\nğŸ’¾ Saving detailed results to {file_name}...")
        df = pd.DataFrame(all_excel_records)
        df.to_excel(file_name, index=False)
        print("âœ… Saved.")

if __name__ == "__main__":
    main()