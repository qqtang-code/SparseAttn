import os
import datasets
import torch
import logging
import ast
import numpy as np
from typing import Dict, Any, List, Tuple
from dataclasses import dataclass, field
from typing import Optional
import glob
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from datasets import load_dataset
import shutil

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# å…¨å±€å¸¸é‡
CLASS_MAP = {
    'Single QA': 0, 
    'MultiHop QA': 1, 
    'Summarization': 2, 
    'Code': 3,
    'In-Context Learning': 4,
}

@dataclass
class PackedDataArguments:
    single_seq: bool = False
    subsplit_length: Optional[int] = None
    per_device_max_tokens: int = 128*1024
    apply_instruct_masks: bool = False
    prepack: bool = False
    streaming: bool = False
    min_seq_len: Optional[int] = 1000
    task_type: str = "pretrain" 
    use_packing: bool = False
    data_cache_dir: Optional[str] = None
    preprocessing_num_workers: int = 32
    suffix: str = "qwen3_8b"

# =========================================================
#  ç‹¬ç«‹çš„å¤„ç†å‡½æ•° (Worker Function)
#  å¿…é¡»æ”¾åœ¨é¡¶å±‚ï¼Œä»¥ä¾¿å¤šè¿›ç¨‹åºåˆ—åŒ– (Pickle)
# =========================================================

def _process_single_item(item, tokenizer, class_map, is_sft=False):
    """å¤„ç†å•æ¡æ•°æ®ä¸º token ids (æ— æˆªæ–­)"""
    ctx = item.get("context", "") or ""
    q = item.get("question", "") or ""
    a_text = item.get("answer", "") or ""

    # ä¿®å¤ï¼šç§»é™¤ UTF-8 BOM å¤´ï¼Œé˜²æ­¢å‡ºç° <|begin_of_text|>Ã¯Â»Â¿...
    if isinstance(ctx, str): ctx = ctx.replace('\ufeff', '')
    if isinstance(q, str): q = q.replace('\ufeff', '')
    if isinstance(a_text, str): a_text = a_text.replace('\ufeff', '')

    meta = item.get("metadata", {}) or {}
    task_type = "Other"
    is_prefix = True
    try:
        meta_dict = ast.literal_eval(meta) if isinstance(meta, str) else meta
        task_type = meta_dict.get('task', 'Other')
        is_prefix = meta_dict.get('is_prefix', True)
    except Exception:
        pass


    separator = "\n\n"


    # Context (Segment ID 1)
    ctx_text = "\n" + ctx.rstrip()
    ctx_ids = tokenizer(ctx_text, add_special_tokens=False)["input_ids"]

    # Question (Segment ID 2)
    q_text = "\n" + q.lstrip()
    q_ids = tokenizer(q_text, add_special_tokens=False)["input_ids"]

    if is_prefix:
        user_text = q_text + "\n" + ctx_text

    else:
        user_text = ctx_text + "\n" + q_text
        
    if task_type == "Summarization":
        user_text = "You are given several news passages. Write a one-page summary of all news." + user_text + "\n\nSummary:"
    if task_type == "Code":
        user_text = "Please complete the code given below." + user_text

    # A. æ„é€ æ¶ˆæ¯åˆ—è¡¨
    messages = [{"role": "user", "content": user_text}]
    
    # B. å…ˆè®¡ç®— User éƒ¨åˆ†çš„é•¿åº¦ (ç”¨äº range_ids å®šä½å’Œ Mask)
    #    è¿™åŒ…å«äº† BOS + User Header + Content + EOT
    user_part_text = tokenizer.apply_chat_template(messages, tokenize=False, enable_thinking=False)
    user_part_ids = tokenizer(user_part_text, add_special_tokens=False)["input_ids"]
    user_len = len(user_part_ids)

    # C. å¦‚æœæœ‰ Answerï¼Œè¿½åŠ å¹¶ç”Ÿæˆå®Œæ•´åºåˆ—
    if a_text:
        messages.append({"role": "assistant", "content": a_text})
    
    # D. ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´ ID (Single Passï¼Œå¤©ç„¶æ—  Double BOS)
    full_text = tokenizer.apply_chat_template(messages, tokenize=False, enable_thinking=False)
    full_input_ids = tokenizer(full_text, add_special_tokens=False)["input_ids"]

    # --- 4. æ„å»º Labels å’Œ å¤„ç† EOS ---
    labels = list(full_input_ids) # æµ…æ‹·è´
    
    if is_sft:
        # SFTæ¨¡å¼ï¼šMask æ‰ User éƒ¨åˆ† (-100)
        labels[:user_len] = [-100] * user_len
    else:
        # Pretrainæ¨¡å¼ï¼šä¿ç•™ Lossï¼Œåç»­ä¼šå¤„ç† index 0
        pass
    
    if tokenizer.eos_token_id is not None and (not full_input_ids or full_input_ids[-1] != tokenizer.eos_token_id):
        full_input_ids.append(tokenizer.eos_token_id)
        labels.append(tokenizer.eos_token_id)
        
    # User éƒ¨åˆ†çš„ Start/End
    user_text_start = 0
    # æ³¨æ„ï¼šuser_len æ˜¯é•¿åº¦ï¼Œend index æ˜¯ user_len - 1
    user_text_end = user_len - 1
    
    # Assistant éƒ¨åˆ†çš„ Start/End
    if a_text:
        # Assistant ä» User ç»“æŸåçš„ä¸‹ä¸€ä¸ª token å¼€å§‹
        a_start = user_len 
        a_end = len(full_input_ids) - 1
    else:
        # å¦‚æœæ²¡æœ‰ Answerï¼Œa_start æŒ‡å‘æœ«å°¾
        a_start = user_len
        a_end = len(full_input_ids) - 1 # æˆ–è€… user_len - 1

    # Pretrain æ¨¡å¼ä¸‹ Mask ç¬¬ä¸€ä¸ª token (åŸæœ‰é€»è¾‘)
    if not is_sft and len(labels) > 0:
        labels[0] = -100

    # è¿™é‡Œçš„ 0,0 æ˜¯åŸä»£ç é‡Œçš„ special_start/endï¼Œä¿æŒä¸º 0
    range_ids = [0, 0, user_text_start, user_text_end, user_text_start, user_text_end, a_start, a_end]
    class_id = class_map.get(task_type, 5)

    return {
        "input_ids": full_input_ids,
        "labels": labels,
        "task_id": class_id,
        "task_type": task_type,
        "range_ids": range_ids,
    }

def _finalize_pack(tokenizer, input_ids, labels, task_ids, lengths, task_types, range_ids):
    """æ‰“åŒ…æ”¶å°¾ï¼šPaddingå¹¶è½¬æ¢ä¸ºTensorç»“æ„"""
    seq_lengths = [0] + list(np.cumsum(lengths))

    return {
        "input_ids": input_ids,          # List[int]
        "labels": labels,                # List[int]
        "seq_lengths": seq_lengths,      # List[int]
        "task_ids": task_ids,            # List[int]
        "task_type": task_types,         # List[str]
        "range_ids": range_ids,          # List[int] [8]
    }

def worker_pack_chunk(chunk_dataset, tokenizer, max_seq_len, min_seq_len, worker_id, is_sft=False):
    """
    å­è¿›ç¨‹æ‰§è¡Œçš„å‡½æ•°ï¼šå¤„ç†åˆ†é…ç»™å®ƒçš„é‚£ä¸€éƒ¨åˆ†æ•°æ®
    """
    # é‡è¦ï¼šé˜²æ­¢ tokenizer å†…éƒ¨å†æ¬¡å¹¶è¡Œå¯¼è‡´æ­»é”æˆ–æ€§èƒ½ä¸‹é™
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    local_packed_data = []

    # Buffers
    buf_input_ids = []
    buf_labels = []
    buf_task_ids = []    
    buf_lengths = []     
    buf_task_types = []
    buf_range_ids = []

    # éå†å½“å‰ chunk çš„æ•°æ®
    # ä½¿ç”¨ tqdm éœ€è¦æŒ‡å®š position é¿å…å¤šè¿›ç¨‹æ‰“å°æ··ä¹±ï¼Œæˆ–è€…ç›´æ¥å»æ‰
    iterator = chunk_dataset
    if worker_id % 4 == 3:
        iterator = tqdm(chunk_dataset, desc=f"Worker {worker_id} Packing", position=worker_id)

    for item in iterator:
        processed = _process_single_item(item, tokenizer, CLASS_MAP, is_sft)

        p_input_ids = processed["input_ids"]
        p_len = len(p_input_ids)

        if p_len > max_seq_len or p_len < min_seq_len:
            # å•æ¡è¿‡é•¿ç›´æ¥è·³è¿‡ æˆ–è€… å•æ¡å¤ªçŸ­ä¹Ÿè·³è¿‡ï¼ˆCUDA illegal memory accessï¼‰
            continue

        # è´ªå¿ƒæ‰“åŒ…æ£€æŸ¥
        if len(buf_input_ids) + p_len <= max_seq_len:
            buf_input_ids.extend(p_input_ids)
            buf_labels.extend(processed["labels"])
            buf_task_ids.append(processed["task_id"])
            buf_lengths.append(p_len)
            buf_task_types.append(processed["task_type"])
            buf_range_ids.append(processed["range_ids"])
        else:
            # Buffer æ»¡äº†ï¼Œfinalize
            packed_item = _finalize_pack(tokenizer, buf_input_ids, buf_labels, buf_task_ids, buf_lengths, buf_task_types, buf_range_ids)
            local_packed_data.append(packed_item)

            # Reset buffer
            buf_input_ids = list(p_input_ids)
            buf_labels = list(processed["labels"])
            buf_task_ids = [processed["task_id"]]
            buf_lengths = [p_len]
            buf_task_types = [processed["task_type"]]
            buf_range_ids = [processed["range_ids"]]

    # å¤„ç†æœ€åä¸€ä¸ª buffer
    if buf_input_ids:
        packed_item = _finalize_pack(tokenizer, buf_input_ids, buf_labels, buf_task_ids, buf_lengths, buf_task_types, buf_range_ids)
        local_packed_data.append(packed_item)

    return local_packed_data

# =========================================================
#  ä¸» Dataset ç±»
# =========================================================

class PackedDataset(Dataset):
    def __init__(self, raw_dataset, tokenizer, max_seq_len=128*1024, min_seq_len=1000, 
    cache_dir=None, num_proc=8, raw_path = None, suffix: str = None,
    is_sft: bool = False):
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.min_seq_len = min_seq_len
        self.packed_data = None

        # ç¼“å­˜é€»è¾‘
        self.cache_path = None
        # suffix = os.path.basename(tokenizer.name_or_path.rstrip("/"))
        
        suffix = suffix.lower()
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)
            cache_filename = f"{os.path.basename(raw_path)}_{suffix}_packed_maxseq{max_seq_len}.parquet"
            self.cache_path = os.path.join(cache_dir, cache_filename)
            print(f"*** ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼š{self.cache_path} ***")

        if self.cache_path and os.path.exists(self.cache_path):
            print(f"ğŸš€ å‘ç°ç¼“å­˜æ–‡ä»¶: {self.cache_path}")
            try:
                self.packed_data = load_dataset("parquet", data_files=self.cache_path, split="train",
                                                )
                print(f"âœ… æˆåŠŸåŠ è½½ Parquet ç¼“å­˜! åŒ…å« {len(self.packed_data)} æ¡åºåˆ—ã€‚")
                return 
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥ ({e})ï¼Œå‡†å¤‡é‡æ–°æ‰“åŒ…...")

        print(f"å¼€å§‹å¤šè¿›ç¨‹ Packing... ç›®æ ‡é•¿åº¦: {max_seq_len}, è¿›ç¨‹æ•°: {num_proc}")

        # å¤šè¿›ç¨‹å¤„ç†ï¼Œå¾—åˆ°ä¸€ä¸ªå·¨å¤§çš„ List[Dict]
        packed_data_list = self._parallel_pack_dataset(raw_dataset, num_proc)
        
        keys = ["input_ids", "labels", "seq_lengths", "task_ids", "task_type", "range_ids"]
        columnar = {k: [] for k in keys}
        for item in packed_data_list:
            for k in keys:
                columnar[k].append(item[k])

        print("æ­£åœ¨è½¬æ¢ä¸º HuggingFace Dataset å¯¹è±¡...")
        #self.packed_data = datasets.Dataset.from_list(packed_data_list)
        self.packed_data = datasets.Dataset.from_dict(columnar)

        # ä¿å­˜æœ€ç»ˆç¼“å­˜
        if self.cache_path:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ Parquet åˆ°: {self.cache_path} ...")
            try:
                self.packed_data.to_parquet(self.cache_path) 
                print("âœ… Parquet ä¿å­˜æˆåŠŸ!")
            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def _parallel_pack_dataset(self, raw_dataset, num_proc):
        total_size = len(raw_dataset)
        num_proc = min(num_proc, total_size)
        if num_proc < 1: num_proc = 1

        print(f"Splitting dataset into {num_proc} chunks...")

        chunks = []
        for i in range(num_proc):
            chunks.append(raw_dataset.shard(num_shards=num_proc, index=i, contiguous=True))

        # æäº¤ä»»åŠ¡
        futures = []
        with ProcessPoolExecutor(max_workers=num_proc) as executor:
            for i, chunk in enumerate(chunks):
                futures.append(
                    executor.submit(worker_pack_chunk, chunk, self.tokenizer, self.max_seq_len, self.min_seq_len, i)
                )
        print(f"æ‰€æœ‰å­è¿›ç¨‹å¤„ç†å®Œæ¯•ï¼Œå¼€å§‹æ±‡æ€»æ•°æ®...")

        results = []
        for f in tqdm(as_completed(futures), total=len(futures), desc="Waiting for workers"):
            try:
                res = f.result()
                results.extend(res)
            except Exception as e:
                logger.error(f"Worker failed with error: {e}")
                raise e

        print(f"å¤šè¿›ç¨‹ Packing å®Œæˆã€‚åŸå§‹: {total_size} -> Packed: {len(results)}")
        return results

    def __len__(self):
        return len(self.packed_data)

    def __getitem__(self, idx):
        # HF Dataset é»˜è®¤è¿”å› Python Listï¼Œè¿™é‡Œå¯ä»¥ä¸è½¬ Tensorï¼Œ
        # ç•™ç»™ Collator è½¬ï¼Œæˆ–è€…åœ¨è¿™é‡Œè½¬ã€‚å»ºè®®åœ¨è¿™é‡Œè½¬ï¼Œä¿æŒæ—§æ¥å£ä¹ æƒ¯ã€‚
        item = self.packed_data[idx]
        return {
            "input_ids": torch.tensor(item["input_ids"], dtype=torch.long),
            "labels": torch.tensor(item["labels"], dtype=torch.long),
            "seq_lengths": torch.tensor(item["seq_lengths"], dtype=torch.int32),
            "task_ids": torch.tensor(item["task_ids"], dtype=torch.long),
            "task_type": item["task_type"], # å­—ç¬¦ä¸²åˆ—è¡¨ä¿æŒåŸæ ·
            "range_ids": torch.tensor(item["range_ids"], dtype=torch.long),
        }

# =========================================================
#  Utilities & Main
# =========================================================


def build_packed_dataset(paths: str, data_args, tokenizer=None, is_sft: bool =False):
    # if isinstance(paths, str):
    #     paths = [paths]

    parquet_files = []
    # for p in paths:
    if os.path.isdir(paths):
        parquet_files.extend(glob.glob(os.path.join(paths, "*.parquet")))
    elif os.path.isfile(paths):
        parquet_files.append(paths)
    

    print(f"******** {parquet_files} *******")
    if not parquet_files:
        raise ValueError("No parquet files found")

    # Load raw
    raw = load_dataset(
        "parquet", 
        data_files=parquet_files, 
        split="train", 
        cache_dir=os.path.join(data_args.data_cache_dir, "raw") if data_args.data_cache_dir else None
    )
    
    # def filter_fn(x):
    #     task_type = x.get("metadata", {}).get('task', 'Other')
    #     if task_type == "Summarization" or task_type == "Code":
    #         return False
    #     return task_type == "Single QA" or task_type == "MultiHop QA"
    # raw = raw.filter(filter_fn, num_proc=os.cpu_count())

    # 2. æ£€æŸ¥å¹¶è®¡ç®— length å­—æ®µ (å¦‚æœåŸæ•°æ®æ²¡æœ‰)
    if "length" not in raw.column_names:
        print("Extracting 'length' from metadata for sorting...")

        # è¿™é‡Œçš„ int() å¾ˆé‡è¦ï¼š
        # 1. ä½ çš„ JSON ç¤ºä¾‹é‡Œ length æ˜¯å­—ç¬¦ä¸² ("length": "")
        # 2. å¦‚æœä¸è½¬ intï¼Œæ’åºä¼šæŒ‰å­—å…¸åº ("10" æ’åœ¨ "2" å‰é¢)ï¼Œå¯¼è‡´æ‰“åŒ…æ•ˆç‡å˜å·®
        raw = raw.map(
            lambda x: {"length": int(x["metadata"]["length"]) if x["metadata"]["length"] else 0},
            num_proc=data_args.preprocessing_num_workers,
            desc="Extracting lengths"
        )

    # 3. æŒ‰ç…§ length ä»å°åˆ°å¤§æ’åº
    print("ğŸ“‰ æ­£åœ¨æŒ‰ length ä»å°åˆ°å¤§æ’åºæ•°æ®...")
    raw = raw.sort("length", reverse=False)

    max_len = data_args.per_device_max_tokens
    min_len = data_args.min_seq_len

    # å®ä¾‹åŒ–å¹¶è§¦å‘å¤šè¿›ç¨‹å¤„ç†
    return PackedDataset(
        raw, 
        tokenizer, 
        max_seq_len=max_len, # æ ¹æ®éœ€è¦è°ƒæ•´,
        min_seq_len=min_len,
        cache_dir=data_args.data_cache_dir,
        num_proc=data_args.preprocessing_num_workers, # ä½¿ç”¨å‚æ•°æ§åˆ¶æ ¸æ•°
        raw_path=paths,
        suffix=data_args.suffix,
        is_sft=is_sft,
    )


if __name__ == "__main__":
    # 1. å¤šè¿›ç¨‹å¯åŠ¨æ–¹å¼è®¾ç½® (CUDAç¯å¢ƒå¿…å¤‡)
    multiprocessing.set_start_method("spawn", force=True) 

    # 2. é…ç½®å‚æ•°
    # å»ºè®®å…ˆç”¨å°æ•°æ®æˆ–å°‘é‡ worker æµ‹è¯•ï¼Œè·‘é€šåå†è°ƒå¤§
    path = "/data2/public_data/qwen_mix_sft_64K6" 
    data_args = PackedDataArguments(
        preprocessing_num_workers=32,
        data_cache_dir="/data2/public_data/data_cache",
        per_device_max_tokens=65536,
        min_seq_len=1000,
        suffix="qwen3-8b_new",
    )

    # 3. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained("/data2/hf_models/Qwen3-8B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 4. æ„å»º/åŠ è½½æ•°æ®é›† (è‡ªåŠ¨è§¦å‘ æ’åº -> Packing -> Parquetä¿å­˜)
    import time
    print(f"\nâ±ï¸  Start building dataset...")
    
    start_time = time.time()
    dataset = build_packed_dataset(
        paths=path,
        data_args=data_args,
        tokenizer=tokenizer,
        is_sft=False,
    )
    end_time = time.time()
    elapsed = end_time - start_time
    print(f"â±ï¸  Done! Total time cost: {elapsed:.2f} s")

    print(f"\nâœ… Dataset ready. Size: {len(dataset)}")

    # 5. ã€éªŒè¯ç¯èŠ‚ 1ã€‘æ£€æŸ¥å•æ¡æ•°æ®
    # æ³¨æ„ï¼šæ ¹æ® PackedDataset.__getitem__ çš„å®ç°ï¼Œè¿™é‡Œæ‰“å°å‡ºæ¥çš„åº”è¯¥æ˜¯ Tensor
    item0 = dataset[0]
    print("\n--- Sample 0 Check ---")
    print(f"Keys: {item0.keys()}")
    print(f"Input IDs Shape: {item0['input_ids'].shape}")
    print(f"Task Types: {item0['task_type']}")
    print(f"Seq Lengths (cum): {item0['seq_lengths']}")
    print(f"Range ids: {item0['range_ids']}")


    # breakpoint()

    # import copy

    # print("\nâœ‚ï¸  æ­£åœ¨æ‹†åˆ†æ•°æ®é›†...")
    
    # total_len = len(dataset)
    # split_num = 9600

    # if total_len < split_num:
    #     raise ValueError(f"æ•°æ®é›†æ€»é•¿åº¦ ({total_len}) å°äº æ‹†åˆ†æ•°é‡ ({split_num})")

    # hf_full_data = dataset.packed_data

    # # 2. ç”Ÿæˆç´¢å¼•åˆ—è¡¨
    # indices_part1 = range(split_num)               # 0 ~ 9599
    # indices_part2 = range(split_num, total_len)    # 9600 ~ end

    # # 3. ä½¿ç”¨ select è¿›è¡Œåˆ‡ç‰‡ (select ä¸ä¼šå¤åˆ¶å†…å­˜ï¼Œé€Ÿåº¦å¾ˆå¿«)
    # hf_part1 = hf_full_data.select(indices_part1)
    # hf_part2 = hf_full_data.select(indices_part2)

    # # 4. åˆ›å»ºæ–°çš„ PackedDataset åŒ…è£…å™¨
    # # ä½¿ç”¨ copy.copy æµ…æ‹·è´åŸå¯¹è±¡ï¼Œä¿ç•™ tokenizerã€max_seq_len ç­‰é…ç½®
    # # ç„¶åæ›¿æ¢å†…éƒ¨çš„ packed_data
    # dataset_part1 = copy.copy(dataset)
    # dataset_part1.packed_data = hf_part1
    
    # dataset_part2 = copy.copy(dataset)
    # dataset_part2.packed_data = hf_part2

    # print(f"âœ… æ‹†åˆ†å®Œæˆ:")
    # print(f"   Part 1 Size: {len(dataset_part1)} (Target: 9600)")
    # print(f"   Part 2 Size: {len(dataset_part2)}")

    # # 5. (å¯é€‰) ä¿å­˜æ‹†åˆ†åçš„æ•°æ®é›†åˆ°ç£ç›˜ï¼Œæ–¹ä¾¿ä¸‹æ¬¡ç›´æ¥åŠ è½½
    # part1_path = f"{path}_split1_{data_args.suffix}_packed_maxseq{data_args.per_device_max_tokens}.parquet"
    # part2_path = f"{path}_split2_{data_args.suffix}_packed_maxseq{data_args.per_device_max_tokens}.parquet"
    # hf_part1.to_parquet(part1_path)
    # hf_part2.to_parquet(part2_path)