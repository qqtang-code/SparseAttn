import os
import torch
import logging
import ast
import numpy as np
from typing import Dict, Any, List, Tuple
from dataclasses import dataclass, field
from typing import Optional
import glob
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from datasets import load_dataset

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# å…¨å±€å¸¸é‡
CLASS_MAP = {
    'Single QA': 0, 
    'MultiHop QA': 1, 
    'Summarization': 2, 
    'Code': 3
}

# =========================================================
#  ç‹¬ç«‹çš„å¤„ç†å‡½æ•° (Worker Function)
#  å¿…é¡»æ”¾åœ¨é¡¶å±‚ï¼Œä»¥ä¾¿å¤šè¿›ç¨‹åºåˆ—åŒ– (Pickle)
# =========================================================

def _process_single_item(item, tokenizer, class_map):
    """å¤„ç†å•æ¡æ•°æ®ä¸º token ids (æ— æˆªæ–­)"""
    ctx = item.get("context", "") or ""
    q = item.get("question", "") or ""
    a = item.get("answer", "") or ""
    meta = item.get("metadata", {}) or {}
    
    if isinstance(meta, str):
        try:
            meta = ast.literal_eval(meta)
        except:
            meta = {}
    
    flag = str(meta.get("flag", "0"))
    task_type = meta.get('task', 'Other')
    class_id = class_map.get(task_type, 4) # 4 for Other

    separator = "\n\n"

    # Context
    if flag == "1" or not ctx:
        ctx_text = ""
    else:
        ctx_text = "\n" + ctx.rstrip()
    ctx_ids = tokenizer(ctx_text, add_special_tokens=False)["input_ids"]

    # Question
    if flag == "1":
        q_text = "\n" + q.lstrip()
    else:
        q_text = "\n" + q.lstrip() if ctx and q else (q.lstrip() if q and not ctx else "")
    q_ids = tokenizer(q_text, add_special_tokens=False)["input_ids"]

    # Answer
    if a:
        a_text = separator + a
        a_ids = tokenizer(a_text, add_special_tokens=False)["input_ids"]
    else:
        a_ids = []

    full_input_ids = []
    # Segment 1: Context
    full_input_ids.extend(ctx_ids)
    # Segment 2: Question
    full_input_ids.extend(q_ids)
    # Segment 3: Answer
    full_input_ids.extend(a_ids)

    # Add EOS
    if tokenizer.eos_token_id is not None and (not full_input_ids or full_input_ids[-1] != tokenizer.eos_token_id):
        full_input_ids.append(tokenizer.eos_token_id) 
        
    labels = list(full_input_ids)
    # (å¦‚æœéœ€è¦mask answerå‰é¢çš„éƒ¨åˆ†ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ é€»è¾‘)

    return {
        "input_ids": full_input_ids,
        "labels": labels,
        "task_id": class_id,
    }

def _finalize_pack(tokenizer, input_ids, labels, task_ids, lengths):
    """æ‰“åŒ…æ”¶å°¾ï¼šPaddingå¹¶è½¬æ¢ä¸ºTensorç»“æ„"""
    curr_len = len(input_ids)
    remainder = curr_len % 8
    if remainder != 0:
        pad_len = 8 - remainder
        pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
        
        input_ids.extend([pad_id] * pad_len)
        labels.extend([-100] * pad_len)
    
    seq_lengths = [0] + list(np.cumsum(lengths))
    
    return {
        "input_ids": torch.tensor(input_ids, dtype=torch.long),
        "labels": torch.tensor(labels, dtype=torch.long),
        "seq_lengths": torch.tensor(seq_lengths, dtype=torch.int32),
        "task_ids": torch.tensor(task_ids, dtype=torch.long),
    }

def worker_pack_chunk(chunk_dataset, tokenizer, max_seq_len, worker_id=0):
    """
    å­è¿›ç¨‹æ‰§è¡Œçš„å‡½æ•°ï¼šå¤„ç†åˆ†é…ç»™å®ƒçš„é‚£ä¸€éƒ¨åˆ†æ•°æ®
    """
    # é‡è¦ï¼šé˜²æ­¢ tokenizer å†…éƒ¨å†æ¬¡å¹¶è¡Œå¯¼è‡´æ­»é”æˆ–æ€§èƒ½ä¸‹é™
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    local_packed_data = []
    
    # Buffers
    buf_input_ids = []
    buf_labels = []
    buf_task_ids = []    
    buf_lengths = []     

    # éå†å½“å‰ chunk çš„æ•°æ®
    # ä½¿ç”¨ tqdm éœ€è¦æŒ‡å®š position é¿å…å¤šè¿›ç¨‹æ‰“å°æ··ä¹±ï¼Œæˆ–è€…ç›´æ¥å»æ‰
    iterator = chunk_dataset
    if worker_id == 0: # åªè®©ç¬¬ä¸€ä¸ªè¿›ç¨‹æ‰“å°è¿›åº¦æ¡ï¼Œæˆ–è€…æ¯ä¸ªéƒ½æ‰“ä½†ä¸æ¢è¡Œ
        iterator = tqdm(chunk_dataset, desc=f"Worker {worker_id} Packing", position=worker_id)
        
    for item in iterator:
        processed = _process_single_item(item, tokenizer, CLASS_MAP)
        
        p_input_ids = processed["input_ids"]
        p_len = len(p_input_ids)

        if p_len > max_seq_len:
            # å•æ¡è¿‡é•¿ç›´æ¥è·³è¿‡
            continue
        
        # è´ªå¿ƒæ‰“åŒ…æ£€æŸ¥
        if len(buf_input_ids) + p_len <= max_seq_len:
            buf_input_ids.extend(p_input_ids)
            buf_labels.extend(processed["labels"])
            buf_task_ids.append(processed["task_id"])
            buf_lengths.append(p_len)
        else:
            # Buffer æ»¡äº†ï¼Œfinalize
            packed_item = _finalize_pack(tokenizer, buf_input_ids, buf_labels, buf_task_ids, buf_lengths)
            local_packed_data.append(packed_item)
            
            # Reset buffer
            buf_input_ids = list(p_input_ids)
            buf_labels = list(processed["labels"])
            buf_task_ids = [processed["task_id"]]
            buf_lengths = [p_len]

    # å¤„ç†æœ€åä¸€ä¸ª buffer
    if buf_input_ids:
        packed_item = _finalize_pack(tokenizer, buf_input_ids, buf_labels, buf_task_ids, buf_lengths)
        local_packed_data.append(packed_item)
        
    return local_packed_data

# =========================================================
#  ä¸» Dataset ç±»
# =========================================================

class PackedDataset(Dataset):
    def __init__(self, raw_dataset, tokenizer, max_seq_len=128*1024, cache_dir=None, num_proc=8):
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.packed_data = []
        
        # ç¼“å­˜é€»è¾‘
        self.cache_path = None
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)
            cache_filename = f"packed_sft_len{len(raw_dataset)}_seq{max_seq_len}.pt"
            self.cache_path = os.path.join(cache_dir, cache_filename)

        if self.cache_path and os.path.exists(self.cache_path):
            logger.info(f"ğŸš€ å‘ç°ç¼“å­˜æ–‡ä»¶: {self.cache_path}")
            try:
                self.packed_data = torch.load(self.cache_path)
                logger.info(f"âœ… æˆåŠŸåŠ è½½ç¼“å­˜! åŒ…å« {len(self.packed_data)} æ¡åºåˆ—ã€‚")
                return 
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥ ({e})ï¼Œå‡†å¤‡é‡æ–°æ‰“åŒ…...")

        logger.info(f"å¼€å§‹å¤šè¿›ç¨‹ Packing... ç›®æ ‡é•¿åº¦: {max_seq_len}, è¿›ç¨‹æ•°: {num_proc}")
        
        # ================= å¤šè¿›ç¨‹å¤„ç†é€»è¾‘ =================
        self._parallel_pack_dataset(raw_dataset, num_proc)
        # ================================================

        if self.cache_path:
            logger.info(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç¼“å­˜åˆ°: {self.cache_path} ...")
            try:
                torch.save(self.packed_data, self.cache_path)
                logger.info("âœ… ç¼“å­˜ä¿å­˜æˆåŠŸ!")
            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def _parallel_pack_dataset(self, raw_dataset, num_proc):
        # 1. åˆ‡åˆ†æ•°æ®é›†
        # HuggingFace dataset æ”¯æŒ .shard(), æˆ–è€…ç®€å•çš„åˆ‡ç‰‡
        # ä¸ºäº†æ›´å‡åŒ€ï¼Œæˆ‘ä»¬æ‰‹åŠ¨è®¡ç®— indices æˆ–ä½¿ç”¨ shard
        total_size = len(raw_dataset)
        
        # ç¡®ä¿è¿›ç¨‹æ•°ä¸è¶…è¿‡æ•°æ®é‡
        num_proc = min(num_proc, total_size)
        if num_proc < 1: num_proc = 1
        
        logger.info(f"Splitting dataset into {num_proc} chunks...")
        
        # ä½¿ç”¨ shard åˆ‡åˆ†ï¼Œè¿™æ˜¯ HF Dataset æœ€é«˜æ•ˆçš„æ–¹å¼ï¼ˆLazyï¼‰
        chunks = []
        for i in range(num_proc):
            # contiguous=True is important for speed on read
            chunks.append(raw_dataset.shard(num_shards=num_proc, index=i, contiguous=True))

        # 2. æäº¤ä»»åŠ¡åˆ°è¿›ç¨‹æ± 
        futures = []
        with ProcessPoolExecutor(max_workers=num_proc) as executor:
            for i, chunk in enumerate(chunks):
                # æäº¤ä»»åŠ¡
                # æ³¨æ„ï¼štokenizer éœ€è¦è¢« pickle ä¼ è¿‡å»ï¼Œé€šå¸¸æ²¡é—®é¢˜
                # raw_dataset çš„ shard ä¹Ÿæ˜¯ lazy çš„ï¼Œä¼ è¾“å¼€é”€å¾ˆå°
                futures.append(
                    executor.submit(worker_pack_chunk, chunk, self.tokenizer, self.max_seq_len, i)
                )
            
            # 3. æ”¶é›†ç»“æœ
            results = []
            for f in tqdm(as_completed(futures), total=len(futures), desc="Waiting for workers"):
                try:
                    res = f.result()
                    results.extend(res)
                except Exception as e:
                    logger.error(f"Worker failed with error: {e}")
                    raise e
        
        self.packed_data = results
        logger.info(f"å¤šè¿›ç¨‹ Packing å®Œæˆã€‚åŸå§‹: {total_size} -> Packed: {len(self.packed_data)}")

    def __len__(self):
        return len(self.packed_data)

    def __getitem__(self, idx):
        return self.packed_data[idx]

# =========================================================
#  Utilities & Main
# =========================================================

@dataclass
class PackedDataArguments:
    per_device_max_tokens: int = 32768
    min_seq_len: Optional[int] = None
    data_cache_dir: Optional[str] = None
    # Add a param for num_proc
    preprocessing_num_workers: int = 64

def build_packed_dataset(paths, data_args, tokenizer=None):
    if isinstance(paths, str):
        paths = [paths]
    
    parquet_files = []
    for p in paths:
        if os.path.isdir(p):
            parquet_files.extend(glob.glob(os.path.join(p, "*.parquet")))
        elif os.path.isfile(p) and p.endswith(".parquet"):
            parquet_files.append(p)
    
    if not parquet_files:
        raise ValueError("No parquet files found")

    # Load raw
    raw = load_dataset(
        "parquet", 
        data_files=parquet_files, 
        split="train", 
        cache_dir=os.path.join(data_args.data_cache_dir, "raw") if data_args.data_cache_dir else None
    )

    # Filter short
    if data_args.min_seq_len is not None:
        # è¿‡æ»¤ä¹Ÿå¯ä»¥è€ƒè™‘å¤šè¿›ç¨‹: raw.filter(..., num_proc=os.cpu_count())
        pass

    max_len = data_args.per_device_max_tokens
    
    # å®ä¾‹åŒ–å¹¶è§¦å‘å¤šè¿›ç¨‹å¤„ç†
    return PackedDataset(
        raw, 
        tokenizer, 
        max_seq_len=128*1024, # æ ¹æ®éœ€è¦è°ƒæ•´
        cache_dir="data_cache",
        num_proc=data_args.preprocessing_num_workers # ä½¿ç”¨å‚æ•°æ§åˆ¶æ ¸æ•°
    )

class PackedDataCollator:
    def __init__(self, tokenizer=None, data_args=None, max_seq_len=None):
        # ä¿ç•™æ¥å£å…¼å®¹æ€§ï¼Œä½†åœ¨ Packing æ¨¡å¼ä¸‹é€šå¸¸ä¸éœ€è¦ padï¼Œå› ä¸ºéƒ½å·²ç» pack æ»¡æˆ– pad å¥½äº†
        self.tokenizer = tokenizer 
        self.data_args = data_args
        self.max_seq_len = max_seq_len

    def __call__(self, batch: List[Dict]):
        # batch æ˜¯ä¸€ä¸ª listï¼ŒåŒ…å«å¤šä¸ª dataset[i] çš„ç»“æœ
        
        # 1. å¤„ç†å®šé•¿ Tensor (Input IDs, Labels)
        # è¿™äº›å·²ç»æ˜¯ padding åˆ° max_seq_len çš„ï¼Œå¯ä»¥ç›´æ¥ stack
        input_ids = torch.stack([item['input_ids'] for item in batch], dim=0)
        labels = torch.stack([item['labels'] for item in batch], dim=0)
        
        # 2. å¤„ç†å˜é•¿ Tensor (seq_lengths, task_ids)
        # å› ä¸ºæ¯ä¸ª packing æ ·æœ¬åŒ…å«çš„å­æ ·æœ¬æ•°é‡ä¸åŒï¼Œæ— æ³•ç›´æ¥ stack
        # ç­–ç•¥ï¼šå¦‚æœæ˜¯ batch_size=1ï¼Œå¯ä»¥ç›´æ¥å–å‡ºæ¥ï¼›å¦‚æœæ˜¯ >1ï¼Œé€šå¸¸ä¿æŒä¸º list æˆ– flatten
        
        seq_lengths = None
        if 'seq_lengths' in batch[0]:
            # ä¿æŒä¸º List[Tensor]ï¼Œäº¤ç»™æ¨¡å‹å†…éƒ¨å¤„ç† (ä¾‹å¦‚ FlashAttn çš„ varlen æ¥å£é€šå¸¸éœ€è¦æŠŠå®ƒ flatten)
            seq_lengths = [item['seq_lengths'] for item in batch]
            
        task_ids = None
        if 'task_ids' in batch[0]:
            task_ids = [item['task_ids'] for item in batch]

        
        res = {
            "input_ids": input_ids,
            "labels": labels,
            "seq_lengths": seq_lengths, # List[Tensor]
            "task_ids": task_ids,       # List[Tensor]
        }

        return res
if __name__ == "__main__":
    # å¤šè¿›ç¨‹å¿…é¡»åœ¨ main block ä¸‹è¿è¡Œ
    multiprocessing.set_start_method("spawn", force=True) # æ¨èåœ¨ CUDA ç¯å¢ƒæˆ–å¤æ‚åº“ä¸­ä½¿ç”¨ spawn

    path = "/data2/public_data/qwen_mix_sft_128K" 
    data_args = PackedDataArguments(preprocessing_num_workers=64) # è®¾ç½®ä¸ºä½ æœºå™¨çš„ CPU æ ¸å¿ƒæ•°
    tokenizer = AutoTokenizer.from_pretrained("/data2/hf_models/Qwen3-4B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    dataset = build_packed_dataset(
        paths=path,
        data_args=data_args,
        tokenizer=tokenizer
    )
    
    print(f"Dataset ready. Size: {len(dataset)}")
    # check one
    # print(dataset[0])