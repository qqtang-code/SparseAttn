import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional

# --- é…ç½®å‚æ•° ---
SOURCE_DIR = "/data1/lcm_lab/sora/LOOM-Eval/benchmarks/General/LongBench/prediction/12.16sp3_templatesteps200_full_xattn_32k_qwen3-8b_wfrozen_LongBench_noise_64k"
OUTPUT_FILE = "/data2/lcm_lab/public_data/Longbench_50/all.parquet"

# ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ 1ã€‘ï¼šé‡‡æ ·é™åˆ¶
# è®¾ç½®ä¸ºä¸€ä¸ªæ•´æ•°ï¼ˆå¦‚ 50ï¼‰è¡¨ç¤ºæ¯ä¸ªæ–‡ä»¶æœ€å¤šå– 50 æ¡ï¼›è®¾ç½®ä¸º None åˆ™è¡¨ç¤ºè·å–å…¨éƒ¨æ•°æ®
MAX_SAMPLES_PER_FILE: Optional[int] = 50 

SKIP_FILES_PREFIX = {
    "triviaqa", "samsum", "lsht","trec",
    "passage_count", "passage_retrieval_en", "passage_retrieval_zh"
}

TASK_MAP = {
    "narrativeqa": "Single QA", "qasper": "Single QA",
    "multifieldqa_en": "Single QA", "multifieldqa_zh": "Single QA",
    "hotpotqa": "MultiHop QA", "2wikimqa": "MultiHop QA",
    "musique": "MultiHop QA", "dureader": "MultiHop QA",
    "gov_report": "Summarization", "qmsum": "Summarization",
    "multi_news": "Summarization", "vcsum": "Summarization",
    "repobench-p": "Code", "lcc": "Code"
}

def get_file_task(filename: str) -> str:
    for prefix, task in TASK_MAP.items():
        if filename.startswith(prefix):
            return task
    return "Other"

def process_data(source_dir: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    processed_records = []
    global_id_counter = 0

    print(f"ğŸ”„ å¼€å§‹å¤„ç†ç›®å½•: {source_dir}")
    if limit:
        print(f"ğŸ“Œ æ¨¡å¼: æ¯ä¸ªä»»åŠ¡æœ€å¤šæå–å‰ {limit} æ¡æ•°æ®ã€‚")
    else:
        print("ğŸ“Œ æ¨¡å¼: æå–æ‰€æœ‰æ•°æ®ã€‚")

    for filename in sorted(os.listdir(source_dir)): # æ’åºä¸€ä¸‹è®©ç»“æœæ›´ç¨³å®š
        if not filename.endswith(".jsonl"):
            continue

        should_skip = any(filename.startswith(prefix) for prefix in SKIP_FILES_PREFIX)
        if should_skip:
            print(f"â¡ï¸ è·³è¿‡æ–‡ä»¶: {filename}")
            continue

        task = get_file_task(filename)
        file_path = os.path.join(source_dir, filename)
        
        # --- ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ 2ã€‘ï¼šæ–‡ä»¶å†…è®¡æ•°å™¨ ---
        file_sample_count = 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å½“å‰æ–‡ä»¶çš„ä¸Šé™
                if limit is not None and file_sample_count >= limit:
                    break

                try:
                    record = json.loads(line)
                except json.JSONDecodeError as e:
                    print(f"ğŸš¨ é”™è¯¯: è§£æ {filename} å¤±è´¥: {e}")
                    continue

                input_text = record.get("input_text", "")
                answers = record.get("answers", [])

                new_record = {
                    "id": str(global_id_counter),
                    "context": input_text,
                    "question": "",
                    "answer": json.dumps(answers, ensure_ascii=False),
                    "metadata": {
                        "flag": "0",
                        "source": filename,
                        "template": "",
                        "context_type": "",
                        "answer_type": "",
                        "length": len(input_text),
                        "task": task,
                        "is_prefix": False
                    },
                    "others": []
                }
                
                processed_records.append(new_record)
                global_id_counter += 1
                file_sample_count += 1 # å¢åŠ è®¡æ•°
        
        print(f"ğŸ“– æ–‡ä»¶ {filename} å¤„ç†å®Œæ¯•ï¼Œæå–äº† {file_sample_count} æ¡ã€‚")

    print(f"âœ… å¤„ç†å®Œæˆã€‚æ€»è®¡ç”Ÿæˆ {len(processed_records)} æ¡è®°å½•ã€‚")
    return processed_records

def save_to_parquet(data: List[Dict[str, Any]], output_path: str):
    df = pd.DataFrame(data)
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    try:
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {output_path}")
        df.to_parquet(output_path, index=False)
        print("ğŸ‰ ä¿å­˜æˆåŠŸ!")
    except Exception as e:
        print(f"ğŸš¨ ä¿å­˜å¤±è´¥: {e}")

if __name__ == "__main__":
    if not os.path.exists(SOURCE_DIR):
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
    else:
        # ä¼ å…¥é™åˆ¶å‚æ•°
        final_data = process_data(SOURCE_DIR, limit=MAX_SAMPLES_PER_FILE)
        
        if final_data:
            save_to_parquet(final_data, OUTPUT_FILE)
        else:
            print("ğŸš« æ— æ•°æ®ç”Ÿæˆã€‚")