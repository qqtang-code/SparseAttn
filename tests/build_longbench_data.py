import os
import json
import pandas as pd
from typing import List, Dict, Any

# --- é…ç½®å‚æ•° ---
# åŸå§‹ JSONL æ–‡ä»¶ç›®å½•
SOURCE_DIR = "/data1/lcm_lab/sora/LOOM-Eval/benchmarks/General/LongBench/prediction/12.11steps125_qwen_mix_sft_32K_xattn_mlp_ctx_q_new_softmax_wfrozen_LongBench_64k"
# æœ€ç»ˆ Parquet æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„
OUTPUT_FILE = "/data2/lcm_lab/public_data/Longbench/all.parquet"

# éœ€è¦è·³è¿‡çš„æ–‡ä»¶åå‰ç¼€ï¼ˆä¸è¯»å–è¿™äº›æ–‡ä»¶ï¼‰
SKIP_FILES_PREFIX = {
    "triviaqa", "samsum", "lsht","trec",
    "passage_count", "passage_retrieval_en", "passage_retrieval_zh"
}

# ä»»åŠ¡åˆ†ç±»æ˜ å°„ (æ–‡ä»¶å -> ä»»åŠ¡ç±»å‹)
TASK_MAP = {
    # Single QA
    "narrativeqa": "Single QA", "qasper": "Single QA",
    "multifieldqa_en": "Single QA", "multifieldqa_zh": "Single QA",
    # MultiHop QA
    "hotpotqa": "MultiHop QA", "2wikimqa": "MultiHop QA",
    "musique": "MultiHop QA", "dureader": "MultiHop QA",
    # Summarization
    "gov_report": "Summarization", "qmsum": "Summarization",
    "multi_news": "Summarization", "vcsum": "Summarization",
    # Code
    "repobench-p": "Code", "lcc": "Code"
}

def get_file_task(filename: str) -> str:
    """æ ¹æ®æ–‡ä»¶åè·å–å¯¹åº”çš„ä»»åŠ¡ç±»å‹ã€‚"""
    for prefix, task in TASK_MAP.items():
        if filename.startswith(prefix):
            return task
    # å¯¹äºå…¶ä»–æœªæ˜ç¡®åˆ†ç±»ä½†æœªè·³è¿‡çš„æ–‡ä»¶ï¼Œç»™ä¸€ä¸ªé»˜è®¤çš„ä»»åŠ¡ç±»å‹
    return "Other"

def process_data(source_dir: str) -> List[Dict[str, Any]]:
    """
    éå†ç›®å½•ï¼Œè¯»å–å’Œå¤„ç† JSONL æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆæ–°çš„æ•°æ®ç»“æ„ã€‚
    """
    processed_records = []
    global_id_counter = 0

    print(f"ğŸ”„ å¼€å§‹å¤„ç†ç›®å½•: {source_dir}")

    # 1. éå†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(source_dir):
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸º JSONL æ ¼å¼
        if not filename.endswith(".jsonl"):
            continue

        file_path = os.path.join(source_dir, filename)

        # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡è¯¥æ–‡ä»¶
        should_skip = False
        for prefix in SKIP_FILES_PREFIX:
            if filename.startswith(prefix):
                should_skip = True
                break
        
        if should_skip:
            print(f"â¡ï¸ è·³è¿‡æ–‡ä»¶: {filename} (åœ¨ SKIP_FILES_PREFIX ä¸­)")
            continue

        # 3. è·å–ä»»åŠ¡ç±»å‹
        task = get_file_task(filename)
        if task == "Other":
             print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶ {filename} æœªæ˜ç¡®åˆ†ç±»ï¼Œä»»åŠ¡ç±»å‹è®¾ç½®ä¸º 'Other'ã€‚")
        
        print(f"ğŸ“– æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}, ä»»åŠ¡ç±»å‹: {task}")

        # 4. é€è¡Œè¯»å– JSONL æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    record = json.loads(line)
                except json.JSONDecodeError as e:
                    print(f"ğŸš¨ é”™è¯¯: è§£ææ–‡ä»¶ {filename} ä¸­çš„ JSONL è¡Œå¤±è´¥: {e}")
                    continue

                # æå–æ‰€éœ€çš„å­—æ®µ
                input_text = record.get("input_text", "")
                answers = record.get("answers", [])

                # 5. æ„é€ æ–°çš„æ•°æ®ç»“æ„
                new_record = {
                    "id": str(global_id_counter),
                    "context": input_text,
                    "question": "",  # åŸå§‹æ•°æ®ä¸­æœªæä¾›ï¼Œç•™ç©ºæˆ–æ ¹æ®éœ€è¦å¡«å……
                    "answer": json.dumps(answers, ensure_ascii=False), # å°† answers åˆ—è¡¨è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
                    "metadata": {
                        "flag": "0",
                        "source": filename, # ä½¿ç”¨æ–‡ä»¶åä½œä¸º source
                        "template": "",
                        "context_type": "",
                        "answer_type": "",
                        "length": len(input_text), # ä½¿ç”¨ input_text çš„é•¿åº¦
                        "task": task,
                        "is_prefix": False # é»˜è®¤è®¾ç½®ä¸º False
                    },
                    "others": [] # é»˜è®¤ç•™ç©ºï¼Œå¯æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–é”®å€¼å¯¹
                }
                
                processed_records.append(new_record)
                global_id_counter += 1

    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆã€‚å…±ç”Ÿæˆ {len(processed_records)} æ¡è®°å½•ã€‚")
    return processed_records

def save_to_parquet(data: List[Dict[str, Any]], output_path: str):
    """
    å°†å¤„ç†åçš„æ•°æ®åˆ—è¡¨è½¬æ¢ä¸º Pandas DataFrame å¹¶ä¿å­˜ä¸º Parquet æ–‡ä»¶ã€‚
    """
    # 1. å°†æ•°æ®åˆ—è¡¨è½¬æ¢ä¸º DataFrame
    # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨å°† 'metadata' å­—æ®µä½œä¸ºä¸€ä¸ªå­—å…¸åˆ—å¤„ç†
    df = pd.DataFrame(data)

    # 2. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 3. å†™å…¥ Parquet æ–‡ä»¶
    try:
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ° Parquet æ–‡ä»¶: {output_path}")
        df.to_parquet(output_path, index=False)
        print("ğŸ‰ æ•°æ®æˆåŠŸä¿å­˜!")
    except Exception as e:
        print(f"ğŸš¨ é”™è¯¯: ä¿å­˜ Parquet æ–‡ä»¶å¤±è´¥: {e}")

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
if __name__ == "__main__":
    if not os.path.exists(SOURCE_DIR):
        print(f"âŒ é”™è¯¯: åŸå§‹ç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
    else:
        # 1. å¤„ç†æ•°æ®
        final_data = process_data(SOURCE_DIR)
        
        # 2. ä¿å­˜æ•°æ®
        if final_data:
            save_to_parquet(final_data, OUTPUT_FILE)
        else:
            print("ğŸš« æ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®ï¼Œè·³è¿‡ä¿å­˜ã€‚")